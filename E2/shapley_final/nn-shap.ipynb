{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525e8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "CONSIDER_FOURIER_VALUES = True\n",
    "ONLY_LOAD_N_ROWS_FROM_DATASETS = None       # one to load all rows\n",
    "SHAPLEY_KERNEL_EXPLAINER_SAMPLES = 200      # Default is 200\n",
    "SHAPLEY_BEESWARM_EXPLAINER_SAMPLES = 1600   # Default is 1600\n",
    "MAX_EVALS = 1200                            # Default is 1200\n",
    "BEESWARM_MAX_DISPLAY = 40                   # Default is 40\n",
    "\n",
    "NN_PATIENCE = 5                             # Default is 5\n",
    "NN_NUM_EPOCHS = 50                          # Default is 200\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "print(\"import matplotlib\")\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "print(\"importing numpy and pandas\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "print(\"importing sklearn\")\n",
    "from loguru import logger\n",
    "from sklearn import pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"importing shap\")\n",
    "import shap\n",
    "\n",
    "print(\"importing nn\")\n",
    "import E2.nn as nn\n",
    "import rename_columns.rename as rename\n",
    "\n",
    "print(\"all imports done...\")\n",
    "# import dotenv\n",
    "\n",
    "# Way to dynamically change the number of jobs at run time\n",
    "def get_num_jobs(default_jobs: int) -> int:\n",
    "    \"\"\"This function provides a way to override the number of jobs specified\n",
    "    in the command line arguments dynamically.\n",
    "    A file called num_jobs.txt can be created and the first line\n",
    "    should contain the number of jobs.\n",
    "\n",
    "    Args:\n",
    "        default_jobs (int): default value if it is not overridden\n",
    "\n",
    "    Returns:\n",
    "        int: number of jobs to run\n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"num_jobs.txt\"):\n",
    "        return default_jobs\n",
    "    with open(\"num_jobs.txt\") as f:\n",
    "        try:\n",
    "            line = f.readlines()[0].strip()\n",
    "            temp_jobs = int(line)\n",
    "            if temp_jobs > 0 and temp_jobs < 20:\n",
    "                logger.info(f\"NUM_JOBS override: {temp_jobs}\")\n",
    "                return temp_jobs\n",
    "        except:\n",
    "            return default_jobs\n",
    "    return default_jobs\n",
    "\n",
    "\n",
    "def random_seed() -> None:\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "\n",
    "def get_save_filename() -> str:\n",
    "    return f\"{str(uuid.uuid4())}.csv.gz\"\n",
    "\n",
    "\n",
    "def gc_collect() -> None:\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            gc.collect(j)\n",
    "\n",
    "\n",
    "def get_columns_and_types(thisdf: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "    \"\"\"For each feature set type, get the relevant columns.\n",
    "\n",
    "    Args:\n",
    "        thisdf (pd.DataFrame): Input dataframe.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[str]]: Dictionary that maps the feature type to the\n",
    "            list of columns to the feature type.\n",
    "    \"\"\"\n",
    "    columns = [c for c in thisdf.columns if not c.startswith(\"an_\")]\n",
    "\n",
    "    def get_columns(columns: List[str], start_string: str) -> List[str]:\n",
    "        columns = [c for c in columns if c.startswith(start_string)]\n",
    "        columns = [c for c in columns if \"head\" not in c and \"tail\" not in c]\n",
    "        columns = [c for c in columns if \"begin\" not in c and \"end\" not in c]\n",
    "        columns = [c for c in columns if \"filesize\" not in c]\n",
    "        columns = [c for c in columns if \"dit.shanon\" not in c]\n",
    "        \n",
    "        # DO WE WANT RAW FOURIER VALUES\n",
    "        if not CONSIDER_FOURIER_VALUES:\n",
    "            columns = [c for c in columns if \"fourier.value\" not in c.lower()]\n",
    "        \n",
    "        return columns\n",
    "\n",
    "    baseline_columns = get_columns(columns, \"baseline\")\n",
    "    advanced_columns = get_columns(columns, \"advanced\")\n",
    "    fourier_columns = get_columns(columns, \"fourier\")\n",
    "    fourier_min_columns = [\n",
    "        \"fourier.stat.1byte.autocorr\",\n",
    "        \"fourier.stat.1byte.mean\",\n",
    "        \"fourier.stat.1byte.std\",\n",
    "        \"fourier.stat.1byte.chisq\",\n",
    "        \"fourier.stat.1byte.moment.2\",\n",
    "        \"fourier.stat.1byte.moment.3\",\n",
    "        \"fourier.stat.1byte.moment.4\",\n",
    "        \"fourier.stat.1byte.moment.5\",\n",
    "    ]\n",
    "\n",
    "    baseline_and_advanced = list(set(baseline_columns + advanced_columns))\n",
    "    baseline_and_fourier = list(set(baseline_columns + fourier_columns))\n",
    "    advanced_and_fourier = list(set(advanced_columns + fourier_columns))\n",
    "    baseline_and_fourier_min = list(set(baseline_columns + fourier_min_columns))\n",
    "    advanced_and_fourier_min = list(set(advanced_columns + fourier_min_columns))\n",
    "\n",
    "    baseline_advanced_fourier = list(\n",
    "        set(baseline_columns + advanced_columns + fourier_columns)\n",
    "    )\n",
    "    baseline_advanced_and_fourier_min = list(\n",
    "        set(baseline_columns + advanced_columns + fourier_min_columns)\n",
    "    )\n",
    "\n",
    "    rv = {\n",
    "        \"baseline-advanced-and-fourier\": baseline_advanced_fourier,\n",
    "    }\n",
    "\n",
    "    #logger.info(f\"Features = {rv}\")\n",
    "\n",
    "    return rv\n",
    "\n",
    "\n",
    "def get_annotation_columns(thisdf: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"List of columns used for annotation.\n",
    "\n",
    "    Args:\n",
    "        thisdf (pd.DataFrame): Input dataframe.\n",
    "\n",
    "    Returns:\n",
    "        _type_: List of columns\n",
    "    \"\"\"\n",
    "    return [c for c in thisdf.columns if c.startswith(\"an_\")]\n",
    "\n",
    "\n",
    "def annotate_df_with_additional_fields(\n",
    "    name: str, dataframe: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add some metadata to each dataframe\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the csv/parquet file\n",
    "        dataframe (pd.DataFrame): Dataframe\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with additional information\n",
    "    \"\"\"\n",
    "    if \"base32\" in name or \"b32\" in name:\n",
    "        dataframe[\"an_is_base32\"] = 1\n",
    "    else:\n",
    "        dataframe[\"an_is_base32\"] = 0\n",
    "    dataframe[\"an_is_base32\"] = dataframe[\"an_is_base32\"].astype(np.bool_)\n",
    "\n",
    "    if \"encrypt\" in name:\n",
    "        dataframe[\"is_encrypted\"] = 1\n",
    "    else:\n",
    "        dataframe[\"is_encrypted\"] = 0\n",
    "    dataframe[\"is_encrypted\"] = dataframe[\"is_encrypted\"].astype(np.bool_)\n",
    "\n",
    "    if \"v1\" in name:\n",
    "        dataframe[\"an_v1_encrypted\"] = 1\n",
    "    else:\n",
    "        dataframe[\"an_v1_encrypted\"] = 0\n",
    "    dataframe[\"an_v1_encrypted\"] = dataframe[\"an_v1_encrypted\"].astype(np.bool_)\n",
    "\n",
    "    if \"v2\" in name:\n",
    "        dataframe[\"an_v2_encrypted\"] = 1\n",
    "    else:\n",
    "        dataframe[\"an_v2_encrypted\"] = 0\n",
    "    dataframe[\"an_v2_encrypted\"] = dataframe[\"an_v2_encrypted\"].astype(np.bool_)\n",
    "\n",
    "    if \"v3\" in name:\n",
    "        dataframe[\"an_v3_encrypted\"] = 1\n",
    "    else:\n",
    "        dataframe[\"an_v3_encrypted\"] = 0\n",
    "    dataframe[\"an_v3_encrypted\"] = dataframe[\"an_v3_encrypted\"].astype(np.bool_)\n",
    "\n",
    "    def is_webp(filename: str) -> int:\n",
    "        return 1 if \".webp\" in filename else 0\n",
    "\n",
    "    dataframe[\"an_is_webp\"] = (\n",
    "        dataframe[\"extended.base_filename\"].map(is_webp).astype(np.bool_)\n",
    "    )\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def load_data(input_directory: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load all pandas data files from a directory and annotate them with\n",
    "    additional fields\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): input directory\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A combined dataframe of all files (Train)\n",
    "        pd.DataFrame: A combined dataframe of all files (Test)\n",
    "    \"\"\"\n",
    "\n",
    "    interesting_files = [\n",
    "        \"plaintext.base32.combined.csv.gz\",\n",
    "        \"expanded.base32.des3.csv.gz\",\n",
    "        \"n1.plaintext.base32.csv.gz\",\n",
    "        \"expanded_encrypted_v3.csv.gz\",\n",
    "        \"n1.expanded.plaintext.csv.gz\",\n",
    "        \"n1.expanded.pyencrypted_v2.csv.gz\",\n",
    "        \"n1.expanded.pyencrypted_v1.base32.csv.gz\",\n",
    "        \"expanded.des3.csv.gz\",\n",
    "        \"n1.expanded.pyencrypted_v2.base32.csv.gz\",\n",
    "        \"expanded.pyencrypted_v1.csv.gz\",\n",
    "        \"expanded.pyencrypted_v2.base32.csv.gz\",\n",
    "        \"expanded_encrypted_v3_base32.csv.gz\",\n",
    "        \"n1.expanded.pyencrypted_v3.base32.csv.gz\",\n",
    "        \"plaintext.combined.csv.gz\",\n",
    "        \"plaintext.expanded.csv.gz\",\n",
    "        \"expanded.pyencrypted_v2.csv.gz\",\n",
    "        \"expanded.plaintext.base32.csv.gz\",\n",
    "        \"n1.expanded.pyencrypted_v3.csv.gz\",\n",
    "        \"n1.expanded.plaintext.base32.csv.gz\",\n",
    "        \"n1.plaintext.csv.gz\",\n",
    "        \"expanded.pyencrypted_v1.b32.csv.gz\",\n",
    "        \"n1.expanded.pyencrypted_v1.csv.gz\",\n",
    "    ]\n",
    "    p = 0.1\n",
    "    logger.info(\"Loading dataframes\")\n",
    "    if ONLY_LOAD_N_ROWS_FROM_DATASETS and ONLY_LOAD_N_ROWS_FROM_DATASETS is not None:\n",
    "        dataframes = {\n",
    "            # f: pd.read_csv(f, skiprows=lambda i: i > 0 and random.random() > p)\n",
    "            f: pd.read_csv(f, nrows=ONLY_LOAD_N_ROWS_FROM_DATASETS)\n",
    "            for f in tqdm.tqdm(\n",
    "                glob.glob(f\"{input_directory}{os.path.sep}*.csv.gz\"), desc=\"Loading data\"\n",
    "            )\n",
    "            if os.path.basename(f).lower() in interesting_files\n",
    "        }\n",
    "    else:\n",
    "        dataframes = {\n",
    "            # f: pd.read_csv(f, skiprows=lambda i: i > 0 and random.random() > p)\n",
    "            f: pd.read_csv(f)\n",
    "            for f in tqdm.tqdm(\n",
    "                glob.glob(f\"{input_directory}{os.path.sep}*.csv.gz\"), desc=\"Loading data\"\n",
    "            )\n",
    "            if os.path.basename(f).lower() in interesting_files\n",
    "        }\n",
    "\n",
    "    logger.info(\"Annotating dataframes with additional fields\")\n",
    "    dataframes = {\n",
    "        f: annotate_df_with_additional_fields(f, df) for f, df in dataframes.items()\n",
    "    }\n",
    "\n",
    "    logger.info(\"Combining test dataframes into a single dataframe\")\n",
    "    test_df = (\n",
    "        pd.concat([df for fname, df in dataframes.items() if \"n1.\" in fname.lower()])\n",
    "        .sample(frac=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    logger.info(\"Combining train dataframes into a single dataframe\")\n",
    "    train_df = (\n",
    "        pd.concat(\n",
    "            [df for fname, df in dataframes.items() if \"n1.\" not in fname.lower()]\n",
    "        )\n",
    "        .sample(frac=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    \n",
    "    logger.info(\"done...\")\n",
    "    return train_df, test_df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_train_df, master_test_df = load_data(\"/Users/phantom/dev/NapierOne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22f8e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annot_columns = get_annotation_columns(master_train_df)\n",
    "fscolumns = get_columns_and_types(master_train_df)[\"baseline-advanced-and-fourier\"]\n",
    "colnames = [c for c in fscolumns if \"is_encrypted\" not in c]\n",
    "colnames = [c for c in colnames if c not in annot_columns]\n",
    "colnames = [c for c in colnames if not c.startswith(\"an_\")]\n",
    "colnames = [c for c in colnames if not c.lower() == \"extended.base_filename\"]\n",
    "            \n",
    "            \n",
    "train_df=master_train_df[colnames + [\"is_encrypted\"]].copy()\n",
    "test_df=master_test_df[colnames + [\"is_encrypted\"]].copy()\n",
    "            \n",
    "print(f\"{train_df.shape=}\")\n",
    "            \n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "X_train = train_df[colnames]\n",
    "y_train = train_df[\"is_encrypted\"].to_numpy().flatten()\n",
    "X_test = test_df[colnames]\n",
    "y_test = test_df[\"is_encrypted\"].to_numpy().flatten()\n",
    "            \n",
    "gc_collect()        \n",
    "print(\"Creating NN MODEL\")\n",
    "pline = nn.NNModel(input_dim=X_train.shape[-1], num_epochs=NN_NUM_EPOCHS, patience=NN_PATIENCE)\n",
    "print(\"alling NN.fit\")\n",
    "pline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddd039",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transforming X_train\")\n",
    "tr_X_train = pline.scalar.transform(X_train)\n",
    "print(\"Transoforming X_test\")\n",
    "#X_test = test_df[colnames]\n",
    "tr_X_test = pline.scalar.transform(X_test)\n",
    "classif_fn = pline.model.predict\n",
    "\n",
    "tr_X_sample = shap.sample(tr_X_train, SHAPLEY_KERNEL_EXPLAINER_SAMPLES)\n",
    "\n",
    "print(\"Creating shap kernel explainer\")\n",
    "explainer = shap.KernelExplainer(classif_fn, tr_X_sample)\n",
    "\n",
    "print(\"Getting shap values\")\n",
    "shap_values = explainer.shap_values(tr_X_sample, nsamples=\"auto\")\n",
    "\n",
    "print(\"Final prediction\")\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    tr_X_test,\n",
    "    feature_names=[rename.rename_column(c) for c in colnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b04eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating shap kernel explainer\")\n",
    "explainer_ = shap.Explainer(\n",
    "    pline.model,\n",
    "    shap.sample(tr_X_train, SHAPLEY_BEESWARM_EXPLAINER_SAMPLES),\n",
    "    max_evals=MAX_EVALS)\n",
    "    #tr_X_train)\n",
    "\n",
    "print(\"Getting shap values\")\n",
    "explanation_ = explainer_(\n",
    "    shap.sample(tr_X_test, SHAPLEY_BEESWARM_EXPLAINER_SAMPLES), # tr_X_train\n",
    "    )\n",
    "\n",
    "explanation2_ = shap.Explanation(\n",
    "    values=explanation_.values, \n",
    "    base_values=explanation_.base_values, \n",
    "    data=explanation_.data,\n",
    "    feature_names=[rename.rename_column(c) for c in colnames]\n",
    ")\n",
    "shap.plots.beeswarm(explanation2_, max_display=BEESWARM_MAX_DISPLAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe93e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class BeeswarmInformation:\n",
    "    original_column_names: List[str]\n",
    "    new_column_names: List[str]\n",
    "    explanation: shap.Explanation\n",
    "    description: str\n",
    "\n",
    "info = BeeswarmInformation(\n",
    "    original_column_names=colnames,\n",
    "    new_column_names=[rename.rename_column(c) for c in colnames],\n",
    "    explanation=explanation2_,\n",
    "    description=\"Neural-Network with fourier PSD Values considered, test use as dataset.\"\n",
    ")\n",
    "\n",
    "with open(\"nn-shap-with-fourier-values-explanation.pkl\", \"wb\") as f:\n",
    "    pickle.dump(info, file=f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating shap kernel explainer\")\n",
    "e2explainer_ = shap.Explainer(pline.model, tr_X_sample, max_evals=MAX_EVALS)\n",
    "\n",
    "print(\"Getting shap values\")\n",
    "e2explanation_ = e2explainer_(tr_X_sample)\n",
    "\n",
    "e2explanation2_ = shap.Explanation(\n",
    "    values=e2explanation_.values, \n",
    "    base_values=e2explanation_.base_values, \n",
    "    data=e2explanation_.data,\n",
    "    feature_names=[rename.rename_column(c) for c in colnames]\n",
    ")\n",
    "shap.plots.beeswarm(explanation2_, max_display=BEESWARM_MAX_DISPLAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aaf36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class BeeswarmInformation:\n",
    "    original_column_names: List[str]\n",
    "    new_column_names: List[str]\n",
    "    explanation: shap.Explanation\n",
    "    description: str\n",
    "\n",
    "info = BeeswarmInformation(\n",
    "    original_column_names=colnames,\n",
    "    new_column_names=[rename.rename_column(c) for c in colnames],\n",
    "    explanation=e2explanation2_,\n",
    "    description=\"Neural-Network with fourier PSD Values considered (train used as dataset).\"\n",
    ")\n",
    "\n",
    "with open(\"nn-shap-with-fourier-values-explanation-2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(info, file=f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
